{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Initialization and Preprocessing Setup\n",
    "\n",
    "This cell sets up the initial stage for data preprocessing, including file handling and structure initialization:\n",
    "\n",
    "1. **Imports Required Libraries**:\n",
    "   - `gzip`: Facilitates reading and writing compressed files in gzip format.\n",
    "   - `json`: Allows parsing and manipulation of JSON data structures.\n",
    "   - `datetime`: Provides tools for working with date and time.\n",
    "\n",
    "2. **Defines File Paths**:\n",
    "   - `input_file`: Specifies the input dataset, a gzipped JSONL file named `training-dataset.jsonl.gz`.\n",
    "   - `output_file`: Specifies the name of the output file, `extracted_data.json`, where processed data will be stored.\n",
    "\n",
    "3. **Initializes Data Structures**:\n",
    "   - `extracted_data`: An empty list intended to hold extracted and processed records from the dataset.\n",
    "   - `day_mapping`: A dictionary mapping weekday names (e.g., \"Monday\") to integers (0–6) for uniform representation.\n",
    "   - `media_type_mapping`: A dictionary that converts media types (e.g., \"VIDEO\", \"IMAGE\") into numeric codes (0–2).\n",
    "\n",
    "4. **Processes the Input File**:\n",
    "   - Opens the gzipped JSONL input file in text mode using `gzip.open`.\n",
    "   - Iterates through each line of the file, assuming each line contains a valid JSON object.\n",
    "   - Parses the JSON data and extracts specific profile-related attributes (e.g., `username`, `follower_count`, `is_private`) for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data written to extracted_data.json\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"training-dataset.jsonl.gz\"\n",
    "output_file = \"extracted_data.json\"\n",
    "\n",
    "# Initialize a list to store the extracted data\n",
    "extracted_data = []\n",
    "\n",
    "day_mapping = {\n",
    "    \"monday\": 0,\n",
    "    \"tuesday\": 1,\n",
    "    \"wednesday\": 2,\n",
    "    \"thursday\": 3,\n",
    "    \"friday\": 4,\n",
    "    \"saturday\": 5,\n",
    "    \"sunday\": 6\n",
    "}\n",
    "\n",
    "media_type_mapping = {\n",
    "    \"VIDEO\": 0,\n",
    "    \"IMAGE\": 1,\n",
    "    \"CAROUSEL_ALBUM\": 2\n",
    "}\n",
    "\n",
    "# Process the gzipped JSONL file\n",
    "with gzip.open(input_file, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)  # Parse each JSON line\n",
    "        \n",
    "        # Extract profile details\n",
    "        profile_data = {\n",
    "            \"username\": record[\"profile\"].get(\"username\"),\n",
    "            \"is_private\": record[\"profile\"].get(\"is_private\"),\n",
    "            \"follower_count\": record[\"profile\"].get(\"follower_count\"),\n",
    "            \"following_count\": record[\"profile\"].get(\"following_count\")\n",
    "        }\n",
    "        \n",
    "        # Extract post details and transform timestamp into day of week and hour interval\n",
    "        like_counts = []\n",
    "        media_types = []\n",
    "        time_indexes = []\n",
    "        for post in record.get(\"posts\", []):\n",
    "            # Parse timestamp\n",
    "            timestamp = post.get(\"timestamp\")\n",
    "            if timestamp:\n",
    "                dt = datetime.strptime(timestamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "                day_of_week = dt.strftime(\"%A\")  # Get the day of the week\n",
    "                day_of_week = day_mapping[day_of_week.lower()]\n",
    "                hour_interval = dt.hour  # Extract the hour (0-23)\n",
    "            else:\n",
    "                day_of_week = None\n",
    "                hour_interval = None\n",
    "            time_index = day_of_week * 24 + hour_interval\n",
    "\n",
    "            time_indexes.append(hour_interval)\n",
    "            media_types.append(media_type_mapping[post.get(\"media_type\")])\n",
    "            like_counts.append(post.get(\"like_count\"))\n",
    "        \n",
    "        # Combine profile and post data\n",
    "        extracted_data.append({\n",
    "            \"profile\": profile_data,\n",
    "            \"time_indexes\": time_indexes,\n",
    "            \"media_types\": media_types,\n",
    "            \"like_counts\":like_counts\n",
    "        })\n",
    "\n",
    "# Save the extracted data to a new JSON file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_data, f, indent=4)\n",
    "\n",
    "print(f\"Extracted data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading, Transformation, and Regression Pipeline Implementation\n",
    "\n",
    "This cell defines two main functions and executes the regression pipeline for modeling and evaluating the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Imports Required Libraries**:\n",
    "   - `json`: Parses JSON data from files.\n",
    "   - `pandas`: Provides tools for data manipulation and analysis.\n",
    "   - `sklearn`: Includes modules for data splitting, machine learning models, and performance metrics.\n",
    "\n",
    "2. **Function 1: `load_and_prepare_data`**:\n",
    "   - **Purpose**: Reads a JSON file (`extracted_data.json`) containing profile-level and post-level data, transforms it into a flattened structure, and returns it as a pandas DataFrame.\n",
    "   - **Key Steps**:\n",
    "     - Reads the JSON data from the file.\n",
    "     - Extracts user-level information (`username`, `is_private`, `follower_count`, etc.).\n",
    "     - Combines post-level details (`time_indexes`, `media_types`, `like_counts`) with user data.\n",
    "     - Converts the processed data into a tabular format using pandas.\n",
    "\n",
    "3. **Function 2: `run_regression_pipeline`**:\n",
    "   - **Purpose**: Executes a complete workflow, including data cleaning, feature engineering, model training, and evaluation.\n",
    "   - **Key Steps**:\n",
    "     - Loads the dataset using the `load_and_prepare_data` function.\n",
    "     - Cleans the data (e.g., removes rows with missing `like_count` values).\n",
    "     - Encodes categorical features (`media_type`) using one-hot encoding.\n",
    "     - Splits the data into training and testing sets (80-20 split).\n",
    "     - Trains a `RandomForestRegressor` on the training data.\n",
    "     - Evaluates the model using test data and calculates performance metrics (`Mean Squared Error`, `R² Score`).\n",
    "     - Performs 5-fold cross-validation and reports average metrics.\n",
    "\n",
    "4. **Pipeline Execution**:\n",
    "   - The `run_regression_pipeline` function is called within the `if __name__ == \"__main__\":` block to execute the full pipeline:\n",
    "     - Loads and preprocesses the data.\n",
    "     - Trains the model.\n",
    "     - Evaluates the model and prints the results.\n",
    "     - Returns the trained model for potential reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1051879570.2377697\n",
      "Test R^2: 0.5155767769333439\n",
      "\n",
      "Cross-validation R^2 Scores: [0.52471075 0.03195472 0.47179404 0.24972152 0.61995331]\n",
      "Mean R^2: 0.3796268694344282\n",
      "\n",
      "Cross-validation MSE Scores: [1.03204600e+09 3.46435394e+09 2.37201360e+09 5.06540545e+09\n",
      " 1.24997184e+09]\n",
      "Mean MSE: 2636758165.9296446\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. LOAD & COMBINE THE DATA\n",
    "# ---------------------------------------------------------------------------\n",
    "def load_and_prepare_data(json_path='extracted_data.json'):\n",
    "    \"\"\"\n",
    "    Expects a JSON file containing a list of data blocks, \n",
    "    each with 'profile' and lists of 'time_indexes', 'media_types', 'like_counts'.\n",
    "    \n",
    "    Example structure of one data block:\n",
    "    \n",
    "    {\n",
    "        \"profile\": {\n",
    "            \"username\": \"...\",\n",
    "            \"is_private\": false,\n",
    "            \"follower_count\": ...,\n",
    "            \"following_count\": ...\n",
    "        },\n",
    "        \"time_indexes\": [...],\n",
    "        \"media_types\": [...],\n",
    "        \"like_counts\": [...]\n",
    "    }\n",
    "    \n",
    "    Returns a single DataFrame with columns:\n",
    "        username, is_private, follower_count, following_count,\n",
    "        time_index, media_type, like_count\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data_blocks = json.load(f)  # list of user blocks\n",
    "    \n",
    "    all_rows = []\n",
    "    for block in data_blocks:\n",
    "        profile = block['profile']\n",
    "        follower_count = profile.get('follower_count', 0)\n",
    "        following_count = profile.get('following_count', 0)\n",
    "        is_private = profile.get('is_private', False)\n",
    "        username = profile.get('username', None)\n",
    "        \n",
    "        time_indexes = block.get('time_indexes', [])\n",
    "        media_types = block.get('media_types', [])\n",
    "        like_counts = block.get('like_counts', [])\n",
    "        \n",
    "        # Combine post-level data with user-level data\n",
    "        for t_idx, m_type, likes in zip(time_indexes, media_types, like_counts):\n",
    "            row = {\n",
    "                'username': username,\n",
    "                'is_private': is_private,\n",
    "                'follower_count': follower_count,\n",
    "                'following_count': following_count,\n",
    "                'time_index': t_idx,\n",
    "                'media_type': m_type,\n",
    "                'like_count': likes\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    return df\n",
    "\n",
    "# 2. BUILD A PIPELINE-LIKE WORKFLOW\n",
    "# ---------------------------------------------------------------------------\n",
    "def run_regression_pipeline(json_path='extracted_data.json'):\n",
    "    # A) Load data\n",
    "    df = load_and_prepare_data(json_path)\n",
    "    \n",
    "    # B) Basic cleaning / filtering (optional)\n",
    "    # For example, remove rows with missing like_count\n",
    "    df = df.dropna(subset=['like_count'])\n",
    "    \n",
    "    # Convert is_private to int (if it varies in your dataset)\n",
    "    df['is_private'] = df['is_private'].astype(int)\n",
    "    \n",
    "    # One-hot encode media_type\n",
    "    df = pd.get_dummies(df, columns=['media_type'], prefix='media_type')\n",
    "    \n",
    "    # C) Define features and target\n",
    "    feature_cols = [\n",
    "        'is_private',\n",
    "        'follower_count',\n",
    "        'following_count',\n",
    "        'time_index',\n",
    "        # Add the one-hot columns for media_type\n",
    "        # We don't know how many distinct media_types, so let's just grab them dynamically:\n",
    "    ] + [col for col in df.columns if col.startswith('media_type_')]\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    y = df['like_count']\n",
    "    \n",
    "    # D) Split data into train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # E) Train a Random Forest (example regressor)\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # F) Evaluate on the test set\n",
    "    y_pred = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"Test MSE:\", mse)\n",
    "    print(\"Test R^2:\", r2)\n",
    "    \n",
    "    # G) (Optional) Cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores_r2 = cross_val_score(rf, X, y, cv=kf, scoring='r2')\n",
    "    cv_scores_mse = cross_val_score(rf, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    print(\"\\nCross-validation R^2 Scores:\", cv_scores_r2)\n",
    "    print(\"Mean R^2:\", cv_scores_r2.mean())\n",
    "    print(\"\\nCross-validation MSE Scores:\", -cv_scores_mse)\n",
    "    print(\"Mean MSE:\", -cv_scores_mse.mean())\n",
    "\n",
    "    return rf  # return the trained model if you want further use\n",
    "\n",
    "# 3. RUN THE PIPELINE\n",
    "# ---------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # This will train a model on the entire dataset, evaluate it, and print results\n",
    "    model = run_regression_pipeline(\"extracted_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Machine Learning Pipeline with Random Forest and XGBoost\n",
    "\n",
    "This cell implements a complete machine learning pipeline, including data preprocessing, hyperparameter tuning, and model evaluation using Random Forest and XGBoost regressors.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Imports Required Libraries**:\n",
    "   - `numpy` and `pandas`: Essential tools for numerical operations and data manipulation.\n",
    "   - `sklearn`: Modules for data preprocessing, splitting, model training, evaluation, and hyperparameter tuning.\n",
    "   - `xgboost`: Library for the XGBoost regressor, optimized for gradient boosting.\n",
    "\n",
    "2. **Function 1: `load_and_prepare_data`**:\n",
    "   - **Purpose**: Reads and preprocesses the dataset from a JSON file.\n",
    "   - **Key Steps**:\n",
    "     - Loads user-level and post-level data.\n",
    "     - One-hot encodes the `media_type` feature.\n",
    "     - Drops unnecessary or missing data (`like_count`).\n",
    "     - Outputs a pandas DataFrame with all relevant features.\n",
    "\n",
    "3. **Function 2: `run_random_forest`**:\n",
    "   - **Purpose**: Tunes a Random Forest regressor using `RandomizedSearchCV`.\n",
    "   - **Key Steps**:\n",
    "     - Preprocesses numeric features using a `StandardScaler`.\n",
    "     - Defines a pipeline for scaling and modeling.\n",
    "     - Conducts hyperparameter optimization using cross-validation.\n",
    "     - Returns the best model and search results.\n",
    "\n",
    "4. **Function 3: `run_xgboost`**:\n",
    "   - **Purpose**: Tunes an XGBoost regressor using `RandomizedSearchCV`.\n",
    "   - **Key Steps**:\n",
    "     - Similar preprocessing pipeline as Random Forest.\n",
    "     - Optimizes hyperparameters specific to XGBoost (e.g., learning rate, subsample).\n",
    "     - Returns the best model and search results.\n",
    "\n",
    "5. **Function 4: `run_regression_pipeline`**:\n",
    "   - **Purpose**: Orchestrates the entire machine learning workflow.\n",
    "   - **Key Steps**:\n",
    "     - Loads and preprocesses the dataset.\n",
    "     - Applies a log transformation to the target (`like_count`) for normalization.\n",
    "     - Splits the data into training and testing sets.\n",
    "     - Tunes both Random Forest and XGBoost models.\n",
    "     - Evaluates the best-performing model on the test set using MSE and R² metrics.\n",
    "     - Chooses the better model based on test performance and returns it.\n",
    "\n",
    "6. **Execution (`if __name__ == \"__main__\")**:\n",
    "   - Calls the `run_regression_pipeline` function to execute the complete workflow.\n",
    "   - Prints evaluation metrics and determines the superior model (Random Forest or XGBoost).\n",
    "   - Outputs the best model and its hyperparameter tuning results for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== RANDOM FOREST TUNING ==========\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[Random Forest] Best Params: {'regressor__n_estimators': 200, 'regressor__min_samples_split': 10, 'regressor__min_samples_leaf': 1, 'regressor__max_depth': 30}\n",
      "[Random Forest] Best CV Score (neg MSE): -0.5232709996800945\n",
      "\n",
      "========== XGBOOST TUNING ==========\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:49:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBoost] Best Params: {'regressor__subsample': 1.0, 'regressor__n_estimators': 300, 'regressor__max_depth': 10, 'regressor__learning_rate': 0.3, 'regressor__colsample_bytree': 0.8}\n",
      "[XGBoost] Best CV Score (neg MSE): -0.6163268144830145\n",
      "\n",
      "========== EVALUATING BEST MODELS ON TEST SET ==========\n",
      "[Random Forest] Test MSE: 696320078.4031826\n",
      "[Random Forest] Test R^2: 0.6793229698435448\n",
      "[XGBoost] Test MSE: 812575443.1124936\n",
      "[XGBoost] Test R^2: 0.6257837624430779\n",
      "\n",
      "**Random Forest performed better on the TEST set**\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "###############################################################################\n",
    "# 1. LOADING & PREPARING DATA\n",
    "###############################################################################\n",
    "def load_and_prepare_data(json_path='extracted_data.json'):\n",
    "    \"\"\"\n",
    "    Expects a JSON file containing a list of data blocks,\n",
    "    each with 'profile' and lists of 'time_indexes', 'media_types', 'like_counts'.\n",
    "    \n",
    "    Returns a single DataFrame with columns:\n",
    "        [username, is_private, follower_count, following_count,\n",
    "         time_index, media_type_*, like_count]\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data_blocks = json.load(f)\n",
    "    \n",
    "    all_rows = []\n",
    "    for block in data_blocks:\n",
    "        profile = block['profile']\n",
    "        follower_count = profile.get('follower_count', 0)\n",
    "        following_count = profile.get('following_count', 0)\n",
    "        is_private = profile.get('is_private', False)\n",
    "        username = profile.get('username', None)\n",
    "        \n",
    "        time_indexes = block.get('time_indexes', [])\n",
    "        media_types = block.get('media_types', [])\n",
    "        like_counts = block.get('like_counts', [])\n",
    "        \n",
    "        for t_idx, m_type, likes in zip(time_indexes, media_types, like_counts):\n",
    "            row = {\n",
    "                'username': username,\n",
    "                'is_private': int(is_private),   # convert bool->int\n",
    "                'follower_count': follower_count,\n",
    "                'following_count': following_count,\n",
    "                'time_index': t_idx,\n",
    "                'media_type': m_type,\n",
    "                'like_count': likes\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(all_rows)\n",
    "    \n",
    "    # Drop rows with missing like_count (if any)\n",
    "    df.dropna(subset=['like_count'], inplace=True)\n",
    "    \n",
    "    # One-hot encode media_type\n",
    "    df = pd.get_dummies(df, columns=['media_type'], prefix='media_type')\n",
    "    df = df.drop(columns=[\"username\"])  \n",
    "    \n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# 2. PIPELINE & HYPERPARAM TUNING FOR RANDOM FOREST\n",
    "###############################################################################\n",
    "def run_random_forest(X_train, y_train, numeric_features):\n",
    "    \"\"\"\n",
    "    Perform RandomizedSearchCV on a RandomForestRegressor with \n",
    "    some typical hyperparameters, using 5-fold cross-validation.\n",
    "    \n",
    "    Returns: best_model, search object\n",
    "    \"\"\"\n",
    "    # Define a preprocessing pipeline for numeric columns\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create a pipeline: Preprocessing + Regressor\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid / distributions\n",
    "    param_dist = {\n",
    "        'regressor__n_estimators': [100, 200, 300, 500],\n",
    "        'regressor__max_depth': [None, 10, 20, 30],\n",
    "        'regressor__min_samples_split': [2, 5, 10],\n",
    "        'regressor__min_samples_leaf': [1, 2, 4],\n",
    "    }\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"[Random Forest] Best Params:\", random_search.best_params_)\n",
    "    print(\"[Random Forest] Best CV Score (neg MSE):\", random_search.best_score_)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    return best_model, random_search\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. PIPELINE & HYPERPARAM TUNING FOR XGBoost\n",
    "###############################################################################\n",
    "def run_xgboost(X_train, y_train, numeric_features):\n",
    "    \"\"\"\n",
    "    Perform RandomizedSearchCV on an XGBRegressor with \n",
    "    typical hyperparameters, using 5-fold cross-validation.\n",
    "    \n",
    "    Returns: best_model, search object\n",
    "    \"\"\"\n",
    "    # Preprocessing pipeline\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(random_state=42, use_label_encoder=False,\n",
    "                                   eval_metric='rmse'))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter distributions\n",
    "    param_dist = {\n",
    "        'regressor__n_estimators': [100, 200, 300, 500],\n",
    "        'regressor__max_depth': [3, 5, 7, 10],\n",
    "        'regressor__learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "        'regressor__subsample': [0.6, 0.8, 1.0],\n",
    "        'regressor__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"[XGBoost] Best Params:\", random_search.best_params_)\n",
    "    print(\"[XGBoost] Best CV Score (neg MSE):\", random_search.best_score_)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    return best_model, random_search\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. MAIN WORKFLOW\n",
    "###############################################################################\n",
    "def run_regression_pipeline(json_path='extracted_data.json', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    1. Load data\n",
    "    2. Log-transform the target\n",
    "    3. Train/Test split\n",
    "    4. Hyperparameter tuning for both Random Forest & XGBoost\n",
    "    5. Evaluate best model on test set\n",
    "    \"\"\"\n",
    "    # A) Load & Prepare Data\n",
    "    df = load_and_prepare_data(json_path)\n",
    "    \n",
    "    # B) Define features and target\n",
    "    feature_cols = [c for c in df.columns if c != 'like_count']\n",
    "    X = df[feature_cols]\n",
    "    y = df['like_count']\n",
    "    \n",
    "    # C) Log Transform the target\n",
    "    #    Convert y -> log(y+1) to reduce skew\n",
    "    y_log = np.log1p(y)\n",
    "    \n",
    "    # D) Train/Test Split\n",
    "    X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
    "        X, y_log, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Keep a copy of untransformed y test\n",
    "    _, y_test = train_test_split(\n",
    "        y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # E) Identify numeric features (assuming all are numeric/dummy-coded)\n",
    "    numeric_features = list(X.columns)\n",
    "    \n",
    "    # F) Run Random Forest Tuning\n",
    "    print(\"========== RANDOM FOREST TUNING ==========\")\n",
    "    rf_best_model, rf_search = run_random_forest(X_train, y_train_log, numeric_features)\n",
    "    \n",
    "    # G) Run XGBoost Tuning\n",
    "    print(\"\\n========== XGBOOST TUNING ==========\")\n",
    "    xgb_best_model, xgb_search = run_xgboost(X_train, y_train_log, numeric_features)\n",
    "    \n",
    "    # H) Evaluate each best model on TEST set\n",
    "    #    We'll invert the log transform with np.expm1\n",
    "    # ---------------------------------------------------\n",
    "    print(\"\\n========== EVALUATING BEST MODELS ON TEST SET ==========\")\n",
    "    \n",
    "    # --- Random Forest ---\n",
    "    y_pred_log_rf = rf_best_model.predict(X_test)\n",
    "    y_pred_rf = np.expm1(y_pred_log_rf)\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "    print(\"[Random Forest] Test MSE:\", mse_rf)\n",
    "    print(\"[Random Forest] Test R^2:\", r2_rf)\n",
    "    \n",
    "    # --- XGBoost ---\n",
    "    y_pred_log_xgb = xgb_best_model.predict(X_test)\n",
    "    y_pred_xgb = np.expm1(y_pred_log_xgb)\n",
    "    mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "    print(\"[XGBoost] Test MSE:\", mse_xgb)\n",
    "    print(\"[XGBoost] Test R^2:\", r2_xgb)\n",
    "    \n",
    "    # I) Choose which model to return\n",
    "    # If we only want the best performing model, we can pick it by MSE or R^2:\n",
    "    if mse_xgb < mse_rf:\n",
    "        print(\"\\n**XGBoost performed better on the TEST set**\")\n",
    "        best_model = xgb_best_model\n",
    "        search_obj = xgb_search\n",
    "    else:\n",
    "        print(\"\\n**Random Forest performed better on the TEST set**\")\n",
    "        best_model = rf_best_model\n",
    "        search_obj = rf_search\n",
    "    \n",
    "    return best_model, search_obj\n",
    "\n",
    "###############################################################################\n",
    "# 5. ENTRY POINT\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    model, search_obj = run_regression_pipeline(\"extracted_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Regression Pipeline with Multi-Model Tuning and Comparison\n",
    "\n",
    "This cell implements an enhanced regression pipeline that includes advanced preprocessing, feature engineering, and hyperparameter tuning for multiple models (Random Forest, XGBoost, LightGBM, and CatBoost).\n",
    "\n",
    "---\n",
    "\n",
    "1. **Imports Required Libraries**:\n",
    "   - Libraries for data manipulation (`numpy`, `pandas`) and model evaluation (`sklearn`).\n",
    "   - Advanced machine learning libraries: `xgboost`, `lightgbm`, and `catboost`.\n",
    "\n",
    "2. **Function 1: `load_and_prepare_data`**:\n",
    "   - **Purpose**: Loads and preprocesses the dataset from a JSON file.\n",
    "   - **Key Steps**:\n",
    "     - Extracts features and target from the JSON structure.\n",
    "     - Creates a new feature, `follower_following_ratio`, by dividing follower count by (following count + 1).\n",
    "     - Caps the `like_count` at its 99th percentile to handle outliers.\n",
    "     - One-hot encodes the `media_type` feature.\n",
    "     - Returns a cleaned and feature-engineered DataFrame.\n",
    "\n",
    "3. **Function 2: `tune_and_evaluate_model`**:\n",
    "   - **Purpose**: Conducts hyperparameter tuning for a given model using `RandomizedSearchCV` and evaluates its performance.\n",
    "   - **Key Steps**:\n",
    "     - Creates a pipeline for preprocessing (scaling numeric features) and modeling.\n",
    "     - Uses a parameter grid to search for the best hyperparameters with cross-validation.\n",
    "     - Evaluates the tuned model on the test set and computes metrics (MSE and R²).\n",
    "     - Returns the best model and its performance metrics.\n",
    "\n",
    "4. **Function 3: `run_regression_pipeline`**:\n",
    "   - **Purpose**: Orchestrates the end-to-end workflow, including model comparison.\n",
    "   - **Key Steps**:\n",
    "     - Loads and preprocesses the dataset.\n",
    "     - Defines parameter grids for Random Forest, XGBoost, LightGBM, and CatBoost.\n",
    "     - Tunes each model using `tune_and_evaluate_model` and evaluates performance on a test set.\n",
    "     - Compares the models based on MSE and R² and identifies the best-performing model.\n",
    "     - Outputs the best model and results for further use.\n",
    "\n",
    "5. **Execution (`if __name__ == \"__main__\")**:\n",
    "   - Calls the `run_regression_pipeline` function to execute the full workflow.\n",
    "   - Performs model tuning and comparison.\n",
    "   - Prints the best model based on MSE and its performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TUNING: RandomForest ==========\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "\n",
      "[RandomForest] Best Params:\n",
      "{'regressor__n_estimators': 100, 'regressor__min_samples_split': 20, 'regressor__min_samples_leaf': 1, 'regressor__max_features': None, 'regressor__max_depth': None}\n",
      "[RandomForest] Test MSE: 68289592.3657\n",
      "[RandomForest] Test R^2:  0.8006\n",
      "\n",
      "========== TUNING: XGBoost ==========\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:14:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[XGBoost] Best Params:\n",
      "{'regressor__subsample': 0.8, 'regressor__reg_lambda': 10, 'regressor__reg_alpha': 0, 'regressor__n_estimators': 500, 'regressor__max_depth': 10, 'regressor__learning_rate': 0.05, 'regressor__gamma': 0, 'regressor__colsample_bytree': 0.6}\n",
      "[XGBoost] Test MSE: 69212654.2728\n",
      "[XGBoost] Test R^2:  0.7979\n",
      "\n",
      "========== TUNING: LightGBM ==========\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 802\n",
      "[LightGBM] [Info] Number of data points in the train set: 146466, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 4369.537770\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "[LightGBM] Best Params:\n",
      "{'regressor__subsample': 0.8, 'regressor__reg_lambda': 10, 'regressor__reg_alpha': 0.1, 'regressor__num_leaves': 255, 'regressor__n_estimators': 100, 'regressor__max_depth': -1, 'regressor__learning_rate': 0.1, 'regressor__colsample_bytree': 0.6}\n",
      "[LightGBM] Test MSE: 69001067.0122\n",
      "[LightGBM] Test R^2:  0.7985\n",
      "\n",
      "========== TUNING: CatBoost ==========\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "9 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:26: Can't create train tmp dir: tmp\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\borab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n",
      " -7.52642123e+07 -6.89758827e+07 -6.91266553e+07 -1.27584755e+08\n",
      " -1.00672641e+08 -9.24311419e+07 -1.12130133e+08 -6.93499565e+07\n",
      " -9.73216665e+07 -1.78916747e+08 -7.96395437e+07 -8.91676191e+07\n",
      " -6.91854097e+07 -7.62564480e+07 -9.26182043e+07 -9.69317132e+07\n",
      " -7.26134283e+07 -1.08498909e+08 -8.28676487e+07 -7.23998274e+07\n",
      " -1.02295483e+08 -7.63871354e+07 -7.72260671e+07 -7.00996867e+07\n",
      " -8.10993718e+07 -7.88450371e+07 -6.85932210e+07 -7.14046488e+07\n",
      " -1.41871286e+08 -8.74499985e+07 -9.73461657e+07 -8.99193409e+07\n",
      " -8.23715222e+07 -9.85253598e+07 -6.87817050e+07 -1.81521211e+08\n",
      " -8.55226147e+07 -6.91277066e+07 -7.65801688e+07 -6.99119087e+07\n",
      " -1.79997957e+08 -1.00848771e+08 -6.99819149e+07 -7.04000542e+07\n",
      " -7.60518025e+07 -1.70852894e+08]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CatBoost] Best Params:\n",
      "{'regressor__subsample': 0.8, 'regressor__learning_rate': 0.2, 'regressor__l2_leaf_reg': 20, 'regressor__iterations': 1000, 'regressor__depth': 8}\n",
      "[CatBoost] Test MSE: 70684150.5739\n",
      "[CatBoost] Test R^2:  0.7936\n",
      "\n",
      "======================================\n",
      "** Best Model by MSE: RandomForest **\n",
      "MSE: 68289592.3657, R^2: 0.8006\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Regressors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "###############################################################################\n",
    "# 1. LOADING, FEATURE ENGINEERING, AND OUTLIER HANDLING\n",
    "###############################################################################\n",
    "def load_and_prepare_data(json_path='extracted_data.json'):\n",
    "    \"\"\"\n",
    "    1) Reads a JSON file with data blocks, each containing:\n",
    "        - profile.follower_count, profile.following_count\n",
    "        - time_indexes, media_types, like_counts\n",
    "    2) Creates a DataFrame with:\n",
    "        - 'follower_following_ratio'\n",
    "        - one-hot-encoded 'media_type'\n",
    "        - 'like_count' capped at the 99th percentile\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data_blocks = json.load(f)\n",
    "    \n",
    "    all_rows = []\n",
    "    for block in data_blocks:\n",
    "        profile = block['profile']\n",
    "        follower_count = profile.get('follower_count', 0)\n",
    "        following_count = profile.get('following_count', 0)\n",
    "        is_private = int(profile.get('is_private', False))\n",
    "        username = profile.get('username', None)\n",
    "        \n",
    "        time_indexes = block.get('time_indexes', [])\n",
    "        media_types = block.get('media_types', [])\n",
    "        like_counts = block.get('like_counts', [])\n",
    "        \n",
    "        for t_idx, m_type, likes in zip(time_indexes, media_types, like_counts):\n",
    "            row = {\n",
    "                'username': username,\n",
    "                'is_private': is_private,\n",
    "                'follower_count': follower_count,\n",
    "                'following_count': following_count,\n",
    "                'time_index': t_idx,\n",
    "                'media_type': m_type,\n",
    "                'like_count': likes\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(all_rows)\n",
    "    # Drop rows with missing like_count if any\n",
    "    df.dropna(subset=['like_count'], inplace=True)\n",
    "\n",
    "    # One-hot encode media_type\n",
    "    df = pd.get_dummies(df, columns=['media_type'], prefix='media_type')\n",
    "\n",
    "    # Create an interaction feature: ratio of follower_count / (following_count + 1)\n",
    "    df['follower_following_ratio'] = df['follower_count'] / (df['following_count'] + 1)\n",
    "\n",
    "    # Outlier handling: cap 'like_count' at 99th percentile\n",
    "    cap_value = df['like_count'].quantile(0.99)\n",
    "    df.loc[df['like_count'] > cap_value, 'like_count'] = cap_value\n",
    "\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# 2. DEFINE HELPER FUNCTION TO RUN RANDOMIZED SEARCH ON A GIVEN MODEL\n",
    "###############################################################################\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def tune_and_evaluate_model(model, param_dist, X_train, y_train, X_test, y_test, model_name=\"Model\", n_iter=50):\n",
    "    \"\"\"\n",
    "    Runs RandomizedSearchCV on the given model + param_dist.\n",
    "    - n_iter can be large (e.g., 50 or 100) if you have enough time (~4 hours).\n",
    "    - Uses 5-fold CV, negative MSE as the scoring.\n",
    "    \n",
    "    Returns best_estimator, MSE, R^2 on test set.\n",
    "    \"\"\"\n",
    "    # We'll do a simple pipeline: scaling + model\n",
    "    numeric_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer, list(X_train.columns))],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # We'll use KFold CV with shuffle=True to get robust performance\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_est = random_search.best_estimator_\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n[{model_name}] Best Params:\")\n",
    "    print(random_search.best_params_)\n",
    "    print(f\"[{model_name}] Test MSE: {mse:.4f}\")\n",
    "    print(f\"[{model_name}] Test R^2:  {r2:.4f}\")\n",
    "    \n",
    "    return best_est, mse, r2\n",
    "\n",
    "###############################################################################\n",
    "# 3. MAIN PIPELINE: LOAD DATA, (OPTIONAL) LOG-TRANSFORM, RUN MODELS\n",
    "###############################################################################\n",
    "def run_regression_pipeline(json_path='extracted_data.json'):\n",
    "    df = load_and_prepare_data(json_path)\n",
    "\n",
    "    # Features & Target\n",
    "    # We'll drop 'username' if it's not numeric\n",
    "    X = df.drop(columns=['username', 'like_count'])\n",
    "    y = df['like_count']\n",
    "\n",
    "    # OPTIONAL: log-transform 'like_count' if beneficial\n",
    "    # y = np.log1p(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # If log-transform is used above, remember to apply np.log1p to y_train, y_test\n",
    "    # and for final predictions you'd do np.expm1(...) on the predicted values.\n",
    "\n",
    "    # ========================\n",
    "    # 3.1 Define BIG param grids\n",
    "    # ========================\n",
    "\n",
    "    # A) Random Forest\n",
    "    rf_param_dist = {\n",
    "        'regressor__n_estimators': [100,300, 500,],\n",
    "        'regressor__max_depth': [None, 10, 20,],\n",
    "        'regressor__min_samples_split': [2, 5, 10, 20],\n",
    "        'regressor__min_samples_leaf': [1, 3, 5],\n",
    "        'regressor__max_features': [None, 'sqrt', 'log2', 0.5],\n",
    "    }\n",
    "\n",
    "    # B) XGBoost\n",
    "    xgb_param_dist = {\n",
    "        'regressor__n_estimators': [100, 300, 500],\n",
    "        'regressor__max_depth': [3,6, 10],\n",
    "        'regressor__learning_rate': [0.01,0.05, 0.1,0.2],\n",
    "        'regressor__subsample': [0.6, 0.8, 1.0],\n",
    "        'regressor__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'regressor__gamma': [0, 0.5, 1, 2],\n",
    "        'regressor__reg_alpha': [0, 1, 10],\n",
    "        'regressor__reg_lambda': [1, 3, 10],\n",
    "    }\n",
    "\n",
    "    # C) LightGBM\n",
    "    lgb_param_dist = {\n",
    "        'regressor__n_estimators': [100, 300, 500, 1000],\n",
    "        'regressor__max_depth': [-1, 10, 15, 30],\n",
    "        'regressor__num_leaves': [31, 127, 255],\n",
    "        'regressor__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'regressor__subsample': [0.6, 0.8, 1.0],\n",
    "        'regressor__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'regressor__reg_alpha': [0, 0.1, 1, 5, 10],\n",
    "        'regressor__reg_lambda': [0, 3, 10],\n",
    "    }\n",
    "\n",
    "    # D) CatBoost\n",
    "    cat_param_dist = {\n",
    "        'regressor__iterations': [100, 300, 500, 1000],\n",
    "        'regressor__depth': [4, 5, 6, 7, 8, 9, 10],\n",
    "        'regressor__learning_rate': [0.01, 0.1, 0.15, 0.2],\n",
    "        'regressor__l2_leaf_reg': [1, 5, 20],\n",
    "        'regressor__subsample': [0.6, 0.8, 1.0],\n",
    "    }\n",
    "\n",
    "    # ========================\n",
    "    # 3.2 Set up the models\n",
    "    # ========================\n",
    "    models = {\n",
    "        \"RandomForest\": (\n",
    "            RandomForestRegressor(random_state=42),\n",
    "            rf_param_dist\n",
    "        ),\n",
    "        \"XGBoost\": (\n",
    "            XGBRegressor(\n",
    "                random_state=42,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='rmse'\n",
    "            ),\n",
    "            xgb_param_dist\n",
    "        ),\n",
    "        \"LightGBM\": (\n",
    "            LGBMRegressor(random_state=42),\n",
    "            lgb_param_dist\n",
    "        ),\n",
    "        \"CatBoost\": (\n",
    "            CatBoostRegressor(random_state=42, silent=True),\n",
    "            cat_param_dist\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    # We'll store results for each model\n",
    "    results = []\n",
    "\n",
    "    # We can choose a large n_iter for the random search if we have ~4 hours\n",
    "    N_ITER_SEARCH = 50  # Try 50 or 100 if you have enough time\n",
    "\n",
    "    for model_name, (model, param_dist) in models.items():\n",
    "        print(f\"\\n========== TUNING: {model_name} ==========\")\n",
    "        best_estimator, mse, r2 = tune_and_evaluate_model(\n",
    "            model,\n",
    "            param_dist,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            model_name=model_name,\n",
    "            n_iter=N_ITER_SEARCH\n",
    "        )\n",
    "        results.append((model_name, best_estimator, mse, r2))\n",
    "\n",
    "    # Sort by MSE ascending\n",
    "    results.sort(key=lambda x: x[2])\n",
    "    best_model_name, best_estimator, best_mse, best_r2 = results[0]\n",
    "    \n",
    "    print(\"\\n======================================\")\n",
    "    print(f\"** Best Model by MSE: {best_model_name} **\")\n",
    "    print(f\"MSE: {best_mse:.4f}, R^2: {best_r2:.4f}\")\n",
    "    print(\"======================================\")\n",
    "    \n",
    "    return best_estimator, results\n",
    "\n",
    "###############################################################################\n",
    "# 4. ENTRY POINT\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    best_model, all_results = run_regression_pipeline(\"extracted_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Script for Regression Model\n",
    "\n",
    "This cell implements a script for loading a trained regression model, preparing test data, making predictions, and saving the results in a structured JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "1. **Function 1: `load_trained_model`**:\n",
    "   - **Purpose**: Loads a pre-trained regression model from a pickle file.\n",
    "   - **Key Steps**:\n",
    "     - Reads the model file (e.g., `best_model.pkl`) using Python's `pickle` library.\n",
    "     - Returns the deserialized model object.\n",
    "\n",
    "2. **Function 2: `prepare_test_data`**:\n",
    "   - **Purpose**: Processes the test dataset to match the structure used during training.\n",
    "   - **Key Steps**:\n",
    "     - Reads the test data file (`test-regression-round3.jsonl`) containing posts with features such as `media_type`, `timestamp`, and `id`.\n",
    "     - Maps `media_type` values to numeric codes and applies one-hot encoding (`media_type_0`, `media_type_1`, `media_type_2`).\n",
    "     - Extracts features like `time_index` (based on day of the week and hour) and computes `follower_following_ratio` (defaulting to zero if data is unavailable).\n",
    "     - Ensures all required columns (e.g., one-hot-encoded features) exist in the output DataFrame.\n",
    "     - Outputs a processed DataFrame ready for prediction.\n",
    "\n",
    "3. **Function 3: `predict_like_count`**:\n",
    "   - **Purpose**: Predicts the `like_count` for posts using the trained regression model.\n",
    "   - **Key Steps**:\n",
    "     - Extracts feature columns from the test DataFrame.\n",
    "     - Uses the model's `predict` method to generate predictions.\n",
    "     - Returns a dictionary mapping `post_id` to the predicted `like_count`.\n",
    "\n",
    "4. **Function 4: `main`**:\n",
    "   - **Purpose**: Executes the entire prediction workflow.\n",
    "   - **Key Steps**:\n",
    "     - Loads the trained model (`best_model.pkl`).\n",
    "     - Prepares the test data from `test-regression-round3.jsonl`.\n",
    "     - Makes predictions and stores them in a dictionary.\n",
    "     - Saves the predictions as a JSON object in `prediction-regression-round3.json`.\n",
    "\n",
    "5. **Execution (`if __name__ == \"__main__\")**:\n",
    "   - Calls the `main` function to perform predictions.\n",
    "   - Outputs the prediction results to `prediction-regression-round3.json`.\n",
    "   - Provides debug information during test data preparation for verification purposes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - final df columns before get_dummies: ['post_id', 'is_private', 'follower_count', 'following_count', 'time_index', 'follower_following_ratio', 'media_type']\n",
      "DEBUG - sample rows:\n",
      "              post_id  is_private  follower_count  following_count  time_index  \\\n",
      "0  18299464882193238           0               0                0          60   \n",
      "1  17870639199008459           0               0                0          11   \n",
      "2  17976060503438195           0               0                0          13   \n",
      "3  17980348256173250           0               0                0          10   \n",
      "4  18030944311530609           0               0                0          85   \n",
      "\n",
      "   follower_following_ratio  media_type  \n",
      "0                       0.0           2  \n",
      "1                       0.0           0  \n",
      "2                       0.0           1  \n",
      "3                       0.0           2  \n",
      "4                       0.0           1  \n",
      "DEBUG - final df columns after get_dummies: ['post_id', 'is_private', 'follower_count', 'following_count', 'time_index', 'follower_following_ratio', 'media_type_0', 'media_type_1', 'media_type_2']\n",
      "DEBUG - sample rows post-dummies:\n",
      "              post_id  is_private  follower_count  following_count  time_index  \\\n",
      "0  18299464882193238           0               0                0          60   \n",
      "1  17870639199008459           0               0                0          11   \n",
      "2  17976060503438195           0               0                0          13   \n",
      "3  17980348256173250           0               0                0          10   \n",
      "4  18030944311530609           0               0                0          85   \n",
      "\n",
      "   follower_following_ratio  media_type_0  media_type_1  media_type_2  \n",
      "0                       0.0         False         False          True  \n",
      "1                       0.0          True         False         False  \n",
      "2                       0.0         False          True         False  \n",
      "3                       0.0         False         False          True  \n",
      "4                       0.0         False          True         False  \n",
      "Predictions saved to prediction-regression-round3.json\n"
     ]
    }
   ],
   "source": [
    "# predict_regression_round3.py\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_trained_model(model_path=\"best_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Loads the trained regression model (pipeline) from disk.\n",
    "    \"\"\"\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_test_data(input_file=\"test-regression-round3.jsonl\"):\n",
    "    \"\"\"\n",
    "    Expects lines (or a file) shaped like:\n",
    "    \n",
    "        {\n",
    "          \"id\": \"17893302182329646\",\n",
    "          \"caption\": \"...\",\n",
    "          \"comments_count\": 0,\n",
    "          \"media_type\": \"IMAGE\",   # or \"VIDEO\", \"CAROUSEL_ALBUM\"\n",
    "          \"media_url\": \"...\",\n",
    "          \"timestamp\": \"2023-11-01 12:43:50\",\n",
    "          \"username\": \"some_user\"\n",
    "        }\n",
    "\n",
    "    We produce columns that match training, specifically:\n",
    "      - post_id => from record[\"id\"]\n",
    "      - is_private => 0 if not in test data\n",
    "      - follower_count, following_count => 0 if not in test data\n",
    "      - time_index => day_of_week * 24 + hour from 'timestamp'\n",
    "      - follower_following_ratio => same logic from training\n",
    "      - media_type => numeric code => get_dummies => media_type_0,1,2\n",
    "    \"\"\"\n",
    "\n",
    "    # Map from string-based media_type to numeric code used in training\n",
    "    media_type_map = {\n",
    "        \"VIDEO\": 0,\n",
    "        \"IMAGE\": 1,\n",
    "        \"CAROUSEL_ALBUM\": 2\n",
    "    }\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line_num, line in enumerate(fh, start=1):\n",
    "            record = json.loads(line.strip())\n",
    "\n",
    "            post_id = record.get(\"id\", f\"line_{line_num}\")\n",
    "\n",
    "            # Convert string \"IMAGE\"/\"VIDEO\"/\"CAROUSEL_ALBUM\" to numeric\n",
    "            raw_media_type = record.get(\"media_type\", \"IMAGE\")\n",
    "            numeric_mtype = media_type_map.get(raw_media_type, 1)  # fallback=1 => IMAGE\n",
    "\n",
    "            # Build time_index from 'timestamp' => day_of_week*24 + hour\n",
    "            time_str = record.get(\"timestamp\", \"\")\n",
    "            if time_str:\n",
    "                try:\n",
    "                    dt = datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                    day_of_week = dt.weekday()  # Monday=0, Sunday=6\n",
    "                    hour = dt.hour\n",
    "                    time_index = day_of_week * 24 + hour\n",
    "                except ValueError:\n",
    "                    time_index = 0\n",
    "            else:\n",
    "                time_index = 0\n",
    "\n",
    "            # If test data doesn't provide these, default to 0\n",
    "            follower_count = 0\n",
    "            following_count = 0\n",
    "            ratio = follower_count / (following_count + 1)\n",
    "            is_private = 0\n",
    "\n",
    "            row = {\n",
    "                \"post_id\": post_id,\n",
    "                \"is_private\": is_private,\n",
    "                \"follower_count\": follower_count,\n",
    "                \"following_count\": following_count,\n",
    "                \"time_index\": time_index,\n",
    "                \"follower_following_ratio\": ratio,\n",
    "                \"media_type\": numeric_mtype\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    print(\"DEBUG - final df columns before get_dummies:\", df.columns.tolist())\n",
    "    print(\"DEBUG - sample rows:\\n\", df.head(5))\n",
    "\n",
    "    # One-hot encode the numeric \"media_type\"\n",
    "    df = pd.get_dummies(df, columns=[\"media_type\"], prefix=\"media_type\")\n",
    "\n",
    "    # Ensure columns for all 3 possible types: media_type_0, media_type_1, media_type_2\n",
    "    for col_name in [\"media_type_0\", \"media_type_1\", \"media_type_2\"]:\n",
    "        if col_name not in df.columns:\n",
    "            df[col_name] = 0\n",
    "\n",
    "    print(\"DEBUG - final df columns after get_dummies:\", df.columns.tolist())\n",
    "    print(\"DEBUG - sample rows post-dummies:\\n\", df.head(5))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_like_count(model, df):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - model: the fitted pipeline/regressor\n",
    "      - df: DataFrame with columns used for training + 'post_id'\n",
    "    Returns a dict { post_id: predicted_like_count }\n",
    "    \"\"\"\n",
    "    if \"post_id\" not in df.columns:\n",
    "        raise KeyError(\"No column 'post_id' found in the DataFrame.\")\n",
    "\n",
    "    post_ids = df[\"post_id\"]\n",
    "    feature_cols = [c for c in df.columns if c != \"post_id\"]\n",
    "\n",
    "    X_test = df[feature_cols]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # If log transform was used, do y_pred = np.expm1(y_pred)\n",
    "\n",
    "    results = {}\n",
    "    for i, pid in enumerate(post_ids):\n",
    "        results[str(pid)] = int(y_pred[i])\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) Load the best model\n",
    "    model = load_trained_model(\"best_model.pkl\")\n",
    "\n",
    "    # 2) Prepare test data from a file named \"test-regression-round3.json\"\n",
    "    df_test = prepare_test_data(\"test-regression-round3.jsonl\")\n",
    "\n",
    "    # 3) Predict\n",
    "    results_dict = predict_like_count(model, df_test)\n",
    "\n",
    "    # 4) Write a single JSON object to \"prediction-regression-round3.json\"\n",
    "    with open(\"prediction-regression-round3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        # indent=4 ensures each post_id is on its own line\n",
    "        json.dump(results_dict, f, indent=4)\n",
    "\n",
    "    print(\"Predictions saved to prediction-regression-round3.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
